import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import pandas as pd
from rich.console import Console
from rich.live import Live
from rich.panel import Panel
from rich.layout import Layout
from rich.table import Table
from rich.text import Text
from rich import box
from dataclasses import dataclass
import time
from typing import Set, List, Dict
from collections import defaultdict
import re
from urllib.robotparser import RobotFileParser
import hashlib
from PIL import Image
from io import BytesIO
import requests
import ssl
from concurrent.futures import ThreadPoolExecutor
import xml.etree.ElementTree as ET

@dataclass
class PageSEOData:
    url: str
    status_code: int
    content_type: str
    title: str = ""
    meta_description: str = ""
    h1: List[str] = None
    h2: List[str] = None
    canonical: str = ""
    robots_meta: str = ""
    word_count: int = 0
    content_length: int = 0
    response_time: float = 0
    redirect_url: str = ""
    images: List[Dict] = None
    inlinks: List[str] = None
    outlinks: List[str] = None
    hreflang: Dict[str, str] = None
    schema_org: List[str] = None
    open_graph: Dict[str, str] = None
    twitter_cards: Dict[str, str] = None
    duplicate_content: bool = False
    content_hash: str = ""
    page_rank: float = 1.0  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ PageRank
    
    def __post_init__(self):
        self.h1 = self.h1 or []
        self.h2 = self.h2 or []
        self.images = self.images or []
        self.inlinks = self.inlinks or []
        self.outlinks = self.outlinks or []
        self.hreflang = self.hreflang or {}
        self.schema_org = self.schema_org or []
        self.open_graph = self.open_graph or {}
        self.twitter_cards = self.twitter_cards or {}

class SEOFrogScanner:
    def __init__(self, start_url: str):
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ URL
        if not urlparse(start_url).scheme:
            start_url = f"https://{start_url}"
        self.start_url = start_url
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ (—É–±–∏—Ä–∞–µ–º www –µ—Å–ª–∏ –µ—Å—Ç—å)
        self.domain = urlparse(start_url).netloc.replace('www.', '')
        
        self.visited_urls: Set[str] = set()
        self.pages_data: Dict[str, PageSEOData] = {}
        self.console = Console()
        self.status_counts = defaultdict(int)
        self.current_url = ""
        self.total_scanned = 0
        self.content_hashes = defaultdict(list)
        self.robots_parser = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∞—Ü–∏—è –∞–Ω–∏–º–∞—Ü–∏–∏ –ª—è–≥—É—à–∫–∏
        self.current_frame = 0
        self.frog_frames = [
            r"""
    [green]  ‚ã±(‚óâ·¥•‚óâ)‚ã∞  [/green]
    """,
            r"""
    [green]  ‚ã±(‚óâ‚Äø‚óâ)‚ã∞  [/green]
    """,
            r"""
    [green]  ‚ã±(‚óâ·¥ó‚óâ)‚ã∞  [/green]
    """
        ]
        
        self.headers = {
            'User-Agent': 'SEOFrog/1.0 (+https://example.com/bot)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        self.config = {
            'follow_robots_txt': True,
            'check_images': True,
            'check_css': True,
            'check_js': True,
            'max_depth': 10,
            'respect_canonical': True,
            'find_duplicates': True,
            'check_schema': True,
            'check_social_tags': True,
            'check_hreflang': True,
            'analyze_performance': True,
            'max_response_time': 5,  # seconds
            'min_word_count': 300,
            'main_domain_only': True,  # –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω (–±–µ–∑ –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤)
            'calculate_pagerank': True,  # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π PageRank
            'pagerank_damping': 0.85,  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è –¥–ª—è PageRank
            'pagerank_iterations': 10,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ PageRank
        }

        # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–æ–∫
        self.not_found_urls = []  # –î–ª—è 404 –æ—à–∏–±–æ–∫
        self.error_urls = []      # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫
        self.error_sources = defaultdict(list)  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ—à–∏–±–æ–∫
        self.redirects = {}       # –î–ª—è —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤

        # –î–æ–±–∞–≤–ª—è–µ–º —Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤
        self.logs = []
        self.max_logs = 8  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö –ª–æ–≥–æ–≤

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –æ—à–∏–±–æ–∫
        self.error_log_file = "seo_errors.log"
        # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –ª–æ–≥–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        with open(self.error_log_file, 'w', encoding='utf-8') as f:
            f.write(f"SEO Frog Scanner Error Log - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("-" * 80 + "\n\n")

    async def fetch_robots_txt(self, session):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ robots.txt"""
        robots_url = urljoin(self.start_url, '/robots.txt')
        try:
            async with session.get(robots_url) as response:
                if response.status == 200:
                    content = await response.text()
                    self.robots_parser = RobotFileParser(robots_url)
                    self.robots_parser.set_url(robots_url)
                    self.robots_parser.parse(content.splitlines())
                else:
                    self.log_error(f"robots.txt –Ω–µ –Ω–∞–π–¥–µ–Ω (—Å—Ç–∞—Ç—É—Å {response.status})")
                    self.add_log("robots.txt –Ω–µ –Ω–∞–π–¥–µ–Ω", "warning")
        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ robots.txt: {e}")
            self.add_log("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ robots.txt", "warning")
            self.robots_parser = None

    def can_fetch(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è URL –≤ robots.txt"""
        if not self.config['follow_robots_txt'] or not self.robots_parser:
            return True
        return self.robots_parser.can_fetch(self.headers['User-Agent'], url)

    def get_main_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω –∏–∑ URL (–±–µ–∑ –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤)"""
        parsed = urlparse(url)
        domain_parts = parsed.netloc.replace('www.', '').split('.')
        if len(domain_parts) >= 2:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–≤–µ —á–∞—Å—Ç–∏ –¥–æ–º–µ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, example.com –∏–∑ sub.example.com)
            return '.'.join(domain_parts[-2:])
        return parsed.netloc.replace('www.', '')

    def is_main_domain_only(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –æ—Å–Ω–æ–≤–Ω—ã–º –¥–æ–º–µ–Ω–æ–º (–±–µ–∑ –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤)"""
        if not self.config['main_domain_only']:
            return True
        
        parsed = urlparse(url)
        domain_parts = parsed.netloc.replace('www.', '').split('.')
        
        # –ï—Å–ª–∏ –¥–æ–º–µ–Ω –∏–º–µ–µ—Ç –±–æ–ª–µ–µ 2 —á–∞—Å—Ç–µ–π, —ç—Ç–æ –ø–æ–¥–¥–æ–º–µ–Ω
        if len(domain_parts) > 2:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ç–æ—Ç –∂–µ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω
        main_domain = self.get_main_domain(url)
        return main_domain == self.get_main_domain(self.start_url)

    def calculate_internal_pagerank(self):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π PageRank –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
        if not self.config['calculate_pagerank'] or not self.pages_data:
            return
        
        self.add_log("–ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å—á–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ PageRank...", "info")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º PageRank –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        urls = list(self.pages_data.keys())
        pagerank = {url: 1.0 / len(urls) for url in urls}
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
        transition_matrix = {}
        for url in urls:
            page_data = self.pages_data[url]
            outlinks = [link for link in page_data.outlinks if link in self.pages_data]
            
            if outlinks:
                # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Å –º–µ–∂–¥—É –∏—Å—Ö–æ–¥—è—â–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
                weight_per_link = 1.0 / len(outlinks)
                transition_matrix[url] = {outlink: weight_per_link for outlink in outlinks}
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Å –º–µ–∂–¥—É –≤—Å–µ–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                transition_matrix[url] = {other_url: 1.0 / len(urls) for other_url in urls}
        
        # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —Ä–∞—Å—á–µ—Ç PageRank
        damping_factor = self.config['pagerank_damping']
        iterations = self.config['pagerank_iterations']
        
        for iteration in range(iterations):
            new_pagerank = {}
            
            for url in urls:
                # –§–æ—Ä–º—É–ª–∞ PageRank: PR(p) = (1-d)/N + d * sum(PR(i)/C(i))
                # –≥–¥–µ d - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è, N - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü, C(i) - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
                
                # –ë–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (—Å–ª—É—á–∞–π–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥)
                base_prob = (1 - damping_factor) / len(urls)
                
                # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–∞ –ø–æ —Å—Å—ã–ª–∫–∞–º
                link_prob = 0.0
                for source_url, transitions in transition_matrix.items():
                    if url in transitions:
                        link_prob += pagerank[source_url] * transitions[url]
                
                new_pagerank[url] = base_prob + damping_factor * link_prob
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            total_rank = sum(new_pagerank.values())
            if total_rank > 0:
                for url in urls:
                    new_pagerank[url] /= total_rank
            
            pagerank = new_pagerank
            
            if (iteration + 1) % 5 == 0:
                self.add_log(f"PageRank –∏—Ç–µ—Ä–∞—Ü–∏—è {iteration + 1}/{iterations}", "info")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º PageRank –≤ –¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        for url, rank in pagerank.items():
            self.pages_data[url].page_rank = rank
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ PageRank –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–ø-—Å—Ç—Ä–∞–Ω–∏—Ü
        sorted_pages = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)
        
        self.add_log(f"PageRank —Ä–∞—Å—Å—á–∏—Ç–∞–Ω! –¢–æ–ø-5 —Å—Ç—Ä–∞–Ω–∏—Ü:", "success")
        for i, (url, rank) in enumerate(sorted_pages[:5]):
            short_url = url.split('/')[-1] if url.split('/')[-1] else url.split('/')[-2]
            self.add_log(f"  {i+1}. {short_url}: {rank:.4f}", "info")
        
        return pagerank

    async def analyze_page(self, session: aiohttp.ClientSession, url: str, html: str, response) -> PageSEOData:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å–±–æ—Ä SEO-–¥–∞–Ω–Ω—ã—Ö"""
        start_time = time.time()
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        page_data = PageSEOData(
            url=url,
            status_code=response.status,
            content_type=response.headers.get('content-type', ''),
            response_time=time.time() - start_time
        )

        try:
            # Title –∏ Meta Description
            if soup.title:
                page_data.title = soup.title.string.strip() if soup.title.string else ""
            
            meta_desc = soup.find('meta', {'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                page_data.meta_description = meta_desc['content'].strip()

            # –ó–∞–≥–æ–ª–æ–≤–∫–∏
            page_data.h1 = [h1.get_text(strip=True) for h1 in soup.find_all('h1')]
            page_data.h2 = [h2.get_text(strip=True) for h2 in soup.find_all('h2')]

            # Canonical –∏ Robots meta
            canonical = soup.find('link', {'rel': 'canonical'})
            page_data.canonical = canonical['href'] if canonical else ""
            
            robots_meta = soup.find('meta', {'name': 'robots'})
            page_data.robots_meta = robots_meta['content'] if robots_meta else ""

            # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            text_elements = []
            for tag in ['p', 'div', 'span', 'article']:
                text_elements.extend([elem.get_text(strip=True) for elem in soup.find_all(tag)])
            text_content = ' '.join(text_elements)
            page_data.word_count = len(re.findall(r'\w+', text_content))
            page_data.content_length = len(html)

            # –•–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            content_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
            page_data.content_hash = content_hash
            if content_hash in self.content_hashes and self.content_hashes[content_hash]:
                page_data.duplicate_content = True
            self.content_hashes[content_hash].append(url)

            # –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if self.config['check_images']:
                for img in soup.find_all('img'):
                    img_data = {
                        'src': img.get('src', ''),
                        'alt': img.get('alt', ''),
                        'title': img.get('title', ''),
                    }
                    if img_data['src']:
                        img_data['src'] = urljoin(url, img_data['src'])
                        page_data.images.append(img_data)

            # Schema.org
            if self.config['check_schema']:
                schema_tags = soup.find_all('script', {'type': 'application/ld+json'})
                page_data.schema_org = [tag.string for tag in schema_tags if tag.string]

            # Open Graph
            if self.config['check_social_tags']:
                for og in soup.find_all('meta', attrs={'property': re.compile('^og:')}):
                    page_data.open_graph[og['property']] = og.get('content', '')

            # Twitter Cards
            if self.config['check_social_tags']:
                for twitter in soup.find_all('meta', attrs={'name': re.compile('^twitter:')}):
                    page_data.twitter_cards[twitter['name']] = twitter.get('content', '')

            # Hreflang
            if self.config['check_hreflang']:
                for hreflang in soup.find_all('link', {'rel': 'alternate', 'hreflang': True}):
                    page_data.hreflang[hreflang['hreflang']] = hreflang['href']

            # –°–±–æ—Ä –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∏ –≤–Ω–µ—à–∏—Ö —Å—Å—ã–ª–æ–∫
            for link in soup.find_all('a', href=True):
                href = urljoin(url, link['href'])
                if urlparse(href).netloc == self.domain:
                    page_data.outlinks.append(href)
                else:
                    page_data.inlinks.append(href)

        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {url}: {str(e)}")
            self.add_log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {url}", "error")  # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞

        return page_data

    def generate_seo_table(self) -> Table:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å SEO-–¥–∞–Ω–Ω—ã–º–∏"""
        table = Table(box=box.ROUNDED, title="üîç SEO –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
        table.add_column("URL", style="cyan", width=40)
        table.add_column("–ó–∞–≥–æ–ª–æ–≤–æ–∫", width=30)
        table.add_column("–°—Ç–∞—Ç—É—Å", justify="center")
        table.add_column("H1", width=20)
        table.add_column("–û–ø–∏—Å–∞–Ω–∏–µ", width=30)
        table.add_column("–ü—Ä–æ–±–ª–µ–º—ã", style="red")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        for url, data in list(self.pages_data.items())[-5:]:
            issues = []
            
            if not data.title:
                issues.append("–ù–µ—Ç Title")
            elif len(data.title) > 60:
                issues.append("–î–ª–∏–Ω–Ω—ã–π Title")
                
            if not data.meta_description:
                issues.append("–ù–µ—Ç Meta")
            elif len(data.meta_description) > 160:
                issues.append("–î–ª–∏–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")
                
            if not data.h1:
                issues.append("–ù–µ—Ç H1")
            elif len(data.h1) > 1:
                issues.append("–ú–Ω–æ–≥–æ H1")

            if data.duplicate_content:
                issues.append("–î—É–±–ª–∏–∫–∞—Ç")

            if data.word_count < self.config['min_word_count']:
                issues.append("–ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞")

            status_color = self.get_status_color(data.status_code)
            
            table.add_row(
                Text(url, overflow="ellipsis"),
                Text(data.title[:30] + "..." if data.title else "", overflow="ellipsis"),
                f"[{status_color}]{data.status_code}[/{status_color}]",
                Text(data.h1[0][:20] + "..." if data.h1 else "", overflow="ellipsis"),
                Text(data.meta_description[:30] + "..." if data.meta_description else "", overflow="ellipsis"),
                ", ".join(issues) if issues else "[green]OK[/green]"
            )
        return table

    def generate_stats_table(self) -> Table:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        table = Table(box=box.ROUNDED, title="üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
        table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
        table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", justify="right")

        total_pages = len(self.pages_data)
        issues_count = sum(1 for data in self.pages_data.values() if not data.title or not data.meta_description or not data.h1)
        duplicate_count = sum(1 for data in self.pages_data.values() if data.duplicate_content)
        
        table.add_row("–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü", str(total_pages))
        table.add_row("–°—Ç—Ä–∞–Ω–∏—Ü —Å –æ—à–∏–±–∫–∞–º–∏", f"[red]{issues_count}[/red]")
        table.add_row("404 –æ—à–∏–±–∫–∏", f"[red]{len(self.not_found_urls)}[/red]")
        table.add_row("–†–µ–¥–∏—Ä–µ–∫—Ç—ã", f"[yellow]{len(self.redirects)}[/yellow]")
        table.add_row("–î—É–±–ª–∏–∫–∞—Ç—ã", f"[yellow]{duplicate_count}[/yellow]")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–¥–∞–º –æ—Ç–≤–µ—Ç–∞
        for status, count in sorted(self.status_counts.items()):
            color = self.get_status_color(status)
            table.add_row(f"–°—Ç–∞—Ç—É—Å {status}", f"[{color}]{count}[/{color}]")
        
        table.add_row("–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä", f"{sum(d.content_length for d in self.pages_data.values()) // (total_pages or 1)} –±–∞–π")
        table.add_row("–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞", f"{sum(d.response_time for d in self.pages_data.values()) / (total_pages or 1):.2f} —Å–µ–∫")
        
        return table

    def generate_pagerank_table(self) -> Table:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å —Ç–æ–ø-—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –ø–æ PageRank"""
        table = Table(box=box.ROUNDED, title="üèÜ –¢–æ–ø-10 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ PageRank")
        table.add_column("–†–∞–Ω–≥", style="cyan", justify="center")
        table.add_column("URL", style="blue", width=40)
        table.add_column("PageRank", justify="right")
        table.add_column("–í—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏", justify="center")
        table.add_column("–ò—Å—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏", justify="center")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ PageRank
        sorted_pages = sorted(
            self.pages_data.items(), 
            key=lambda x: x[1].page_rank, 
            reverse=True
        )[:10]

        for i, (url, data) in enumerate(sorted_pages, 1):
            short_url = url.split('/')[-1] if url.split('/')[-1] else url.split('/')[-2]
            if not short_url:
                short_url = url.split('/')[-3] if len(url.split('/')) > 2 else url
            
            table.add_row(
                str(i),
                Text(short_url, overflow="ellipsis"),
                f"{data.page_rank:.4f}",
                str(len(data.inlinks)),
                str(len(data.outlinks))
            )
        
        return table

    def generate_display(self) -> Layout:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        layout = Layout()
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã—Å–æ—Ç—É –¥–ª—è –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏
        layout.split_column(
            Layout(name="header", size=7),
            Layout(name="main", size=20),
            Layout(name="footer", size=3),
            Layout(name="log", size=10)  # –ù–æ–≤–∞—è —Å–µ–∫—Ü–∏—è –¥–ª—è –ª–æ–≥–æ–≤
        )

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ª—è–≥—É—à–∫–æ–π –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        self.current_frame = (self.current_frame + 1) % len(self.frog_frames)
        header_content = Panel(
            f"{self.frog_frames[self.current_frame]}\n"
            f"üåê SEO –ê–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞: {self.domain}\n"
            f"üìë –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {self.total_scanned}",
            title="SEO Frog Scanner",
            style="bold green",
            border_style="green"
        )
        layout["header"].update(header_content)

        # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        main_layout = Layout()
        main_layout.split_row(
            Layout(
                Panel(
                    self.generate_stats_table(),
                    title="üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
                    border_style="blue"
                ),
                size=40
            ),
            Layout(
                Panel(
                    self.generate_seo_table(),
                    title="üîç –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                    border_style="cyan"
                ),
                size=60
            )
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É PageRank, –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if self.pages_data and any(data.page_rank > 0 for data in self.pages_data.values()):
            pagerank_layout = Layout()
            pagerank_layout.split_column(
                main_layout,
                Layout(
                    Panel(
                        self.generate_pagerank_table(),
                        title="üèÜ PageRank –ê–Ω–∞–ª–∏–∑",
                        border_style="green"
                    ),
                    size=12
                )
            )
            layout["main"].update(pagerank_layout)
        else:
            layout["main"].update(main_layout)

        # –§—É—Ç–µ—Ä —Å —Ç–µ–∫—É—â–∏–º URL
        footer_content = Panel(
            f"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {self.current_url}",
            style="bold blue",
            border_style="blue"
        )
        layout["footer"].update(footer_content)

        # –°–µ–∫—Ü–∏—è –ª–æ–≥–æ–≤
        recent_logs = self.get_recent_logs()  # –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ª–æ–≥–æ–≤
        log_content = Panel(
            "\n".join(recent_logs),
            title="üìù –ü–æ—Å–ª–µ–¥–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è",
            border_style="yellow"
        )
        layout["log"].update(log_content)

        return layout
    
    async def scan_site(self, session: aiohttp.ClientSession, live: Live):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∞–π—Ç–∞"""
        async def process_url(url: str, depth: int = 0, source_url: str = None):
            await asyncio.sleep(0.5)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            
            if depth > self.config['max_depth'] or url in self.visited_urls:
                return

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
            parsed_url = urlparse(url)
            if not parsed_url.scheme:
                url = f"https://{url}"
                parsed_url = urlparse(url)
            
            # –û—á–∏—â–∞–µ–º URL
            clean_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
            if parsed_url.query:
                clean_url += f"?{parsed_url.query}"
            
            # –£–±–∏—Ä–∞–µ–º www –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ–º–µ–Ω
            netloc = parsed_url.netloc.replace('www.', '')
            if netloc != self.domain.replace('www.', ''):
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω (–±–µ–∑ –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤)
            if not self.is_main_domain_only(clean_url):
                self.add_log(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–¥–¥–æ–º–µ–Ω: {clean_url}", "warning")
                return
            
            if clean_url in self.visited_urls:
                return

            self.current_url = clean_url
            self.visited_urls.add(clean_url)
            
            try:
                async with session.get(clean_url, headers=self.headers, timeout=30, allow_redirects=True) as response:
                    self.status_counts[response.status] += 1
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
                    if response.history:
                        redirect_chain = ' -> '.join([str(r.status) for r in response.history] + [str(response.status)])
                        self.add_log(f"–†–µ–¥–∏—Ä–µ–∫—Ç: {clean_url} -> {response.url} ({redirect_chain})", "warning")

                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                    if response.status == 404:
                        error_msg = f"404: {clean_url} (–∏—Å—Ç–æ—á–Ω–∏–∫: {source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'})"
                        self.log_error(error_msg)
                        self.add_log("404: " + clean_url[:50] + "...", "error")
                        self.not_found_urls.append({'url': clean_url, 'source': source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'})
                    elif response.status >= 400:
                        error_msg = f"–û—à–∏–±–∫–∞ {response.status}: {clean_url} (–∏—Å—Ç–æ—á–Ω–∏–∫: {source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'})"
                        self.log_error(error_msg)
                        self.add_log(f"–û—à–∏–±–∫–∞ {response.status}: " + clean_url[:50] + "...", "error")
                    else:
                        self.add_log(f"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {clean_url}", "info")

                    content_type = response.headers.get('content-type', '').lower()
                    if not content_type or 'text/html' not in content_type:
                        return

                    html = await response.text()
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    page_data = await self.analyze_page(session, clean_url, html, response)
                    self.pages_data[clean_url] = page_data
                    self.total_scanned += 1
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    live.update(self.generate_display())
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
                    soup = BeautifulSoup(html, 'html.parser')
                    links = set()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
                    
                    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤–æ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
                    for tag_name in ['a', 'link', 'area', 'base']:
                        for link in soup.find_all(tag_name, href=True):
                            href = link.get('href', '').strip()
                            if href and not href.startswith(('#', 'mailto:', 'tel:', 'javascript:', 'data:')):
                                full_url = urljoin(clean_url, href)
                                links.add(full_url)
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏
                    tasks = []
                    for next_url in links:
                        parsed_next = urlparse(next_url)
                        next_domain = parsed_next.netloc.replace('www.', '')
                        if (next_domain == self.domain.replace('www.', '') and 
                            self.can_fetch(next_url) and 
                            next_url not in self.visited_urls and
                            self.is_main_domain_only(next_url)):  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞
                            tasks.append(process_url(next_url, depth + 1, clean_url))
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–µ–±–æ–ª—å—à–∏–º–∏ –≥—Ä—É–ø–ø–∞–º–∏
                    if tasks:
                        for i in range(0, len(tasks), 5):
                            batch = tasks[i:i+5]
                            await asyncio.gather(*batch, return_exceptions=True)
                        
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {clean_url}: {str(e)} (–∏—Å—Ç–æ—á–Ω–∏–∫: {source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'})"
                self.log_error(error_msg)
                self.add_log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: " + clean_url[:50] + "...", "error")
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ URL
        await process_url(self.start_url)

    async def export_results(self):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
        # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç—á–µ—Ç
        main_data = []
        for url, data in self.pages_data.items():
            main_data.append({
                'URL': url,
                '–°—Ç–∞—Ç—É—Å': data.status_code,
                '–ó–∞–≥–æ–ª–æ–≤–æ–∫': data.title,
                '–ú–µ—Ç-–æ–ø–∏—Å–∞–Ω–∏–µ': data.meta_description,
                'H1': ' | '.join(data.h1),
                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤': data.word_count,
                '–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞': f"{data.response_time:.2f}",
                '–î—É–±–ª–∏–∫–∞—Ç': data.duplicate_content,
                'PageRank': f"{data.page_rank:.4f}",
                '–í—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏': len(data.inlinks),
                '–ò—Å—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏': len(data.outlinks),
                '–ü—Ä–æ–±–ª–µ–º—ã': self.get_page_issues(data)
            })

        df_main = pd.DataFrame(main_data)
        df_main.to_excel('seo_–æ—Ç—á–µ—Ç_–æ—Å–Ω–æ–≤–Ω–æ–π.xlsx', index=False)

        # –û—Ç—á–µ—Ç –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        images_data = []
        for url, data in self.pages_data.items():
            for img in data.images:
                images_data.append({
                    'URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã': url,
                    'URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è': img['src'],
                    'Alt —Ç–µ–∫—Å—Ç': img['alt'],
                    '–ó–∞–≥–æ–ª–æ–≤–æ–∫': img['title']
                })

        df_images = pd.DataFrame(images_data)
        df_images.to_excel('seo_–æ—Ç—á–µ—Ç_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.xlsx', index=False)

        # –û—Ç—á–µ—Ç –ø–æ –¥—É–±–ª–∏–∫–∞—Ç–∞–º
        duplicates_data = []
        for content_hash, urls in self.content_hashes.items():
            if len(urls) > 1:
                duplicates_data.extend([{
                    'URL': url,
                    '–•–µ—à': content_hash,
                    '–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤': i + 1
                } for i, url in enumerate(urls)])

        if duplicates_data:
            df_duplicates = pd.DataFrame(duplicates_data)
            df_duplicates.to_excel('seo_–æ—Ç—á–µ—Ç_–¥—É–±–ª–∏–∫–∞—Ç—ã.xlsx', index=False)

        # –û—Ç—á–µ—Ç –ø–æ PageRank
        if self.config['calculate_pagerank'] and self.pages_data:
            pagerank_data = []
            sorted_pages = sorted(
                self.pages_data.items(), 
                key=lambda x: x[1].page_rank, 
                reverse=True
            )
            
            for url, data in sorted_pages:
                pagerank_data.append({
                    'URL': url,
                    'PageRank': data.page_rank,
                    '–í—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏': len(data.inlinks),
                    '–ò—Å—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏': len(data.outlinks),
                    '–ó–∞–≥–æ–ª–æ–≤–æ–∫': data.title,
                    '–°—Ç–∞—Ç—É—Å': data.status_code,
                    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤': data.word_count
                })
            
            df_pagerank = pd.DataFrame(pagerank_data)
            df_pagerank.to_excel('seo_–æ—Ç—á–µ—Ç_pagerank.xlsx', index=False)

        # –û—Ç—á–µ—Ç –ø–æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º
        if self.redirects:
            redirects_data = [
                {
                    '–° URL': data['from'],
                    '–ù–∞ URL': data['to'],
                    '–¶–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤': data['chain']
                }
                for data in self.redirects.values()
            ]
            df_redirects = pd.DataFrame(redirects_data)
            df_redirects.to_excel('seo_–æ—Ç—á–µ—Ç_—Ä–µ–¥–∏—Ä–µ–∫—Ç—ã.xlsx', index=False)

        # –û—Ç—á–µ—Ç –ø–æ –æ—à–∏–±–∫–∞–º —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        if self.error_urls or self.not_found_urls:
            errors_data = []
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ 404 –æ—à–∏–±–æ–∫
            for error in self.not_found_urls:
                sources = self.error_sources.get(error['url'], [])
                errors_data.append({
                    'URL': error['url'],
                    '–¢–∏–ø –æ—à–∏–±–∫–∏': '404 –ù–µ –Ω–∞–π–¥–µ–Ω–æ',
                    '–ò—Å—Ç–æ—á–Ω–∏–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è': error['source'],
                    '–í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏': ' | '.join(sources) if sources else error['source'],
                    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤': len(sources) if sources else 1
                })
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫
            for error in self.error_urls:
                sources = self.error_sources.get(error['url'], [])
                errors_data.append({
                    'URL': error['url'],
                    '–¢–∏–ø –æ—à–∏–±–∫–∏': f"–û—à–∏–±–∫–∞ {error.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}",
                    '–ò—Å—Ç–æ—á–Ω–∏–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è': error['source'],
                    '–í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏': ' | '.join(sources) if sources else error['source'],
                    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤': len(sources) if sources else 1
                })

            df_errors = pd.DataFrame(errors_data)
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è –æ—à–∏–±–æ–∫)
            df_errors = df_errors.sort_values('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤', ascending=False)
            df_errors.to_excel('seo_–æ—Ç—á–µ—Ç_–æ—à–∏–±–∫–∏.xlsx', index=False)

        # –≠–∫—Å–ø–æ—Ä—Ç –≤ XML
        self.export_to_xml()

    def export_to_xml(self):
        """–≠–∫—Å–ø–æ—Ä—Ç —Å—Ç—Ä–∞–Ω–∏—Ü, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∏–ª—å—Ç—Ä—É, –≤ XML —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ sitemap"""
        urlset = ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
        
        for url, data in self.pages_data.items():
            # Check if the URL is a filter page
            if self.is_filter_page(url):
                url_elem = ET.SubElement(urlset, "url")
                ET.SubElement(url_elem, "loc").text = url
                # Optionally add more elements like lastmod, changefreq, priority
                # ET.SubElement(url_elem, "lastmod").text = "2023-10-01"
                # ET.SubElement(url_elem, "changefreq").text = "monthly"
                # ET.SubElement(url_elem, "priority").text = "0.8"

        tree = ET.ElementTree(urlset)
        tree.write("sitemap.xml", encoding='utf-8', xml_declaration=True)

    def is_filter_page(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π —Ñ–∏–ª—å—Ç—Ä–∞"""
        # Example pattern check for filter pages
        # Adjust the pattern to match your specific filter page URLs
        return "/catalog/pamyatniki/" in url and "dvoynoy" in url

    def get_page_issues(self, data: PageSEOData) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        issues = []
        if not data.title:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç Title")
        elif len(data.title) > 60:
            issues.append("Title —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π")
        
        if not data.meta_description:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç Meta Description")
        elif len(data.meta_description) > 160:
            issues.append("Meta Description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π")
        
        if not data.h1:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç H1")
        elif len(data.h1) > 1:
            issues.append("–ù–µ—Å–∫–æ–ª—å–∫–æ H1 —Ç–µ–≥–æ–≤")
        
        if data.duplicate_content:
            issues.append("–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç")
        
        if data.word_count < self.config['min_word_count']:
            issues.append("–ú–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        
        if data.response_time > self.config['max_response_time']:
            issues.append("–ú–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç")
        
        return " | ".join(issues)

    def get_status_color(self, status_code: int) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–≤–µ—Ç –¥–ª—è —Å—Ç–∞—Ç—É—Å –∫–æ–¥–∞"""
        if status_code < 300:
            return "green"
        elif status_code < 400:
            return "yellow"
        elif status_code < 500:
            return "red"
        else:
            return "red bold"

    def add_log(self, message: str, level: str = "info"):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –ª–æ–≥"""
        timestamp = time.strftime("%H:%M:%S")
        color = {
            "info": "blue",
            "warning": "yellow",
            "error": "red",
            "success": "green"
        }.get(level, "white")
        
        log_entry = f"[{color}]{timestamp} | {message}[/{color}]"
        self.logs.append(log_entry)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö—Ä–∞–Ω–∏–º—ã—Ö –ª–æ–≥–æ–≤
        if len(self.logs) > self.max_logs:
            self.logs.pop(0)

    def get_recent_logs(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ª–æ–≥–æ–≤"""
        return self.logs

    def log_error(self, message: str):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –æ—à–∏–±–∫—É –≤ –ª–æ–≥-—Ñ–∞–π–ª"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        with open(self.error_log_file, 'a', encoding='utf-8') as f:
            f.write(f"[{timestamp}] {message}\n")

    async def run(self):
        """–ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.console.clear()
        self.add_log(f"–ù–∞—á–∞–ª–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∞–π—Ç–∞: {self.domain}", "info")
        
        timeout = aiohttp.ClientTimeout(total=60, connect=10)
        connector = aiohttp.TCPConnector(limit=10, force_close=True, enable_cleanup_closed=True)
        
        try:
            with Live(self.generate_display(), refresh_per_second=2, screen=True) as live:
                async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
                    await self.fetch_robots_txt(session)
                    await self.scan_site(session, live)

                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π PageRank –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
                    if self.config['calculate_pagerank']:
                        self.calculate_internal_pagerank()

                    await self.export_results()
            
            self.add_log("–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!", "success")
            self.add_log(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {self.total_scanned}", "success")
            self.console.print("\n[green]–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ![/green]")
            self.console.print(f"[blue]–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {self.total_scanned}[/blue]")
            self.console.print("\n[blue]–û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª—ã:[/blue]")
            for report in [
                "seo_–æ—Ç—á–µ—Ç_–æ—Å–Ω–æ–≤–Ω–æ–π.xlsx",
                "seo_–æ—Ç—á–µ—Ç_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.xlsx",
                "seo_–æ—Ç—á–µ—Ç_–¥—É–±–ª–∏–∫–∞—Ç—ã.xlsx",
                "seo_–æ—Ç—á–µ—Ç_—Ä–µ–¥–∏—Ä–µ–∫—Ç—ã.xlsx",
                "seo_–æ—Ç—á–µ—Ç_–æ—à–∏–±–∫–∏.xlsx",
                "sitemap.xml"  # Added sitemap XML report
            ]:
                self.console.print(f"- {report}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—á–µ—Ç PageRank, –µ—Å–ª–∏ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω
            if self.config['calculate_pagerank'] and self.pages_data:
                self.console.print("- seo_–æ—Ç—á–µ—Ç_pagerank.xlsx")
            
            self.console.print(f"- {self.error_log_file} (–ª–æ–≥ –æ—à–∏–±–æ–∫)")
        
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
            self.log_error(error_msg)
            self.add_log("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏", "error")
            self.console.print(f"[red]{error_msg}[/red]")
        finally:
            await connector.close()




if __name__ == "__main__":
    website_url = input("–í–≤–µ–¥–∏—Ç–µ URL —Å–∞–π—Ç–∞ –¥–ª—è SEO –∞–Ω–∞–ª–∏–∑–∞: ")
    scanner = SEOFrogScanner(website_url)
    asyncio.run(scanner.run())