import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import pandas as pd
from rich.console import Console
from rich.live import Live
from rich.panel import Panel
from rich.layout import Layout
from rich.table import Table
from rich.text import Text
from rich import box
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn
from dataclasses import dataclass
import time
from typing import Set, List, Dict
from collections import defaultdict
import re
from urllib.robotparser import RobotFileParser
import hashlib
from PIL import Image
from io import BytesIO
import requests
import ssl
from concurrent.futures import ThreadPoolExecutor
import xml.etree.ElementTree as ET

@dataclass
class PageSEOData:
    url: str
    status_code: int
    content_type: str
    title: str = ""
    meta_description: str = ""
    h1: List[str] = None
    h2: List[str] = None
    canonical: str = ""
    robots_meta: str = ""
    word_count: int = 0
    content_length: int = 0
    response_time: float = 0
    redirect_url: str = ""
    images: List[Dict] = None
    inlinks: List[str] = None
    outlinks: List[str] = None
    hreflang: Dict[str, str] = None
    schema_org: List[str] = None
    open_graph: Dict[str, str] = None
    twitter_cards: Dict[str, str] = None
    duplicate_content: bool = False
    content_hash: str = ""
    page_rank: float = 1.0
    internal_links_count: int = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –ù–ê —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É
    
    def __post_init__(self):
        self.h1 = self.h1 or []
        self.h2 = self.h2 or []
        self.images = self.images or []
        self.inlinks = self.inlinks or []
        self.outlinks = self.outlinks or []
        self.hreflang = self.hreflang or {}
        self.schema_org = self.schema_org or []
        self.open_graph = self.open_graph or {}
        self.twitter_cards = self.twitter_cards or {}

class SEOFrogScanner:
    def __init__(self, start_url: str):
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ URL
        if not urlparse(start_url).scheme:
            start_url = f"https://{start_url}"
        self.start_url = start_url
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞
        parsed = urlparse(start_url)
        self.domain = parsed.netloc.lower().replace('www.', '')
        self.main_domain = self.get_main_domain(start_url)
        
        self.visited_urls: Set[str] = set()
        self.pages_data: Dict[str, PageSEOData] = {}
        self.console = Console()
        self.status_counts = defaultdict(int)
        self.current_url = ""
        self.total_scanned = 0
        self.content_hashes = defaultdict(list)
        self.robots_parser = None
        self.internal_links_graph = defaultdict(set)  # –ì—Ä–∞—Ñ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.save_interval = 500  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ 500 —Å—Å—ã–ª–æ–∫
        self.last_save_count = 0
        self.estimated_total_urls = 0  # –û—Ü–µ–Ω–∫–∞ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ URL
        self.scan_start_time = None
        self.progress_data = {
            'scanned': 0,
            'found': 0,
            'errors': 0,
            'start_time': None,
            'estimated_completion': None
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∏–º–∞—Ü–∏–∏ –ª—è–≥—É—à–∫–∏
        self.current_frame = 0
        self.frog_frames = [
            r"""
    [green]  ‚ã±(‚óâ·¥•‚óâ)‚ã∞  [/green]
    """,
            r"""
    [green]  ‚ã±(‚óâ‚Äø‚óâ)‚ã∞  [/green]
    """,
            r"""
    [green]  ‚ã±(‚óâ·¥ó‚óâ)‚ã∞  [/green]
    """
        ]
        
        self.headers = {
            'User-Agent': 'SEOFrog/1.0 (+https://example.com/bot)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        self.config = {
            'follow_robots_txt': True,
            'check_images': True,
            'check_css': True,
            'check_js': True,
            'max_depth': 10,
            'respect_canonical': True,
            'find_duplicates': True,
            'check_schema': True,
            'check_social_tags': True,
            'check_hreflang': True,
            'analyze_performance': True,
            'max_response_time': 5,
            'min_word_count': 300,
            'main_domain_only': True,
            'calculate_pagerank': True,
            'pagerank_damping': 0.85,
            'pagerank_iterations': 20,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        }

        # –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–æ–∫
        self.not_found_urls = []
        self.error_urls = []
        self.error_sources = defaultdict(list)
        self.redirects = {}

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.logs = []
        self.max_logs = 8
        self.error_log_file = "seo_errors.log"
        
        # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –ª–æ–≥–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        with open(self.error_log_file, 'w', encoding='utf-8') as f:
            f.write(f"SEO Frog Scanner Error Log - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("-" * 80 + "\n\n")

    def get_main_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω –∏–∑ URL (–±–µ–∑ –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤ –∏ www)"""
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace('www.', '')
        domain_parts = domain.split('.')
        
        # –î–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–æ–º–µ–Ω–æ–≤ —Ç–∏–ø–∞ .com.ru, .net.ru –∏ —Ç.–¥.
        if len(domain_parts) >= 3 and domain_parts[-2] in ['com', 'net', 'org', 'edu', 'gov']:
            return '.'.join(domain_parts[-3:])
        elif len(domain_parts) >= 2:
            return '.'.join(domain_parts[-2:])
        return domain

    def is_main_domain_only(self, url: str) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞"""
        if not self.config['main_domain_only']:
            return True
        
        parsed = urlparse(url)
        url_domain = parsed.netloc.lower().replace('www.', '')
        url_main_domain = self.get_main_domain(url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ç–æ—Ç –∂–µ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω
        if url_main_domain != self.main_domain:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ—Ç –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤ (–∫—Ä–æ–º–µ www)
        url_parts = url_domain.split('.')
        main_parts = self.main_domain.split('.')
        
        # –ï—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å—Ç–µ–π –±–æ–ª—å—à–µ, —á–µ–º —É –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞, —ç—Ç–æ –ø–æ–¥–¥–æ–º–µ–Ω
        return len(url_parts) <= len(main_parts)

    def normalize_url(self, url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è"""
        parsed = urlparse(url)
        
        # –£–±–∏—Ä–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (#)
        normalized = f"{parsed.scheme}://{parsed.netloc.lower()}{parsed.path}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        if parsed.query:
            normalized += f"?{parsed.query}"
        
        # –£–±–∏—Ä–∞–µ–º trailing slash –¥–ª—è –Ω–µ-–∫–æ—Ä–Ω–µ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        if normalized.endswith('/') and normalized.count('/') > 3:
            normalized = normalized[:-1]
        
        return normalized

    def build_internal_links_graph(self):
        """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ PageRank"""
        self.internal_links_graph = defaultdict(set)
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
        for page_data in self.pages_data.values():
            page_data.internal_links_count = 0
        
        for url, page_data in self.pages_data.items():
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∏—Å—Ö–æ–¥—è—â–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            for outlink in page_data.outlinks:
                normalized_outlink = self.normalize_url(outlink)
                if normalized_outlink in self.pages_data:
                    self.internal_links_graph[url].add(normalized_outlink)
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
                    self.pages_data[normalized_outlink].internal_links_count += 1

    def update_internal_links_for_page(self, page_url: str, page_data: PageSEOData):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –≥—Ä–∞—Ñ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        outgoing_count = 0
        incoming_count = 0
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏ –æ—Ç —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        for outlink in page_data.outlinks:
            normalized_outlink = self.normalize_url(outlink)
            if normalized_outlink in self.pages_data:
                self.internal_links_graph[page_url].add(normalized_outlink)
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
                self.pages_data[normalized_outlink].internal_links_count += 1
                outgoing_count += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∏ –ù–ê —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç –¥—Ä—É–≥–∏—Ö —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        for other_url, other_data in self.pages_data.items():
            if other_url != page_url:
                for outlink in other_data.outlinks:
                    normalized_outlink = self.normalize_url(outlink)
                    if normalized_outlink == page_url:
                        self.internal_links_graph[other_url].add(page_url)
                        page_data.internal_links_count += 1
                        incoming_count += 1
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)
        if len(self.pages_data) <= 10:
            short_url = self.get_short_url(page_url)
            self.add_log(f"üîó {short_url}: {outgoing_count} –∏—Å—Ö–æ–¥—è—â–∏—Ö, {incoming_count} –≤—Ö–æ–¥—è—â–∏—Ö", "info")

    def calculate_internal_pagerank(self):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ PageRank"""
        if not self.config['calculate_pagerank'] or not self.pages_data:
            return

        self.add_log("–°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫...", "info")
        self.build_internal_links_graph()
        
        self.add_log("–ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å—á–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ PageRank...", "info")
        
        urls = list(self.pages_data.keys())
        n_pages = len(urls)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º PageRank —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ
        pagerank = {url: 1.0 / n_pages for url in urls}
        damping_factor = self.config['pagerank_damping']
        iterations = self.config['pagerank_iterations']
        
        # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —Ä–∞—Å—á–µ—Ç PageRank
        for iteration in range(iterations):
            new_pagerank = {}
            
            for target_url in urls:
                # –ë–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (random surfer model)
                base_rank = (1 - damping_factor) / n_pages
                
                # –†–∞–Ω–≥ –æ—Ç –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
                incoming_rank = 0.0
                
                for source_url in urls:
                    if target_url in self.internal_links_graph[source_url]:
                        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫ —Å –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        outgoing_count = len(self.internal_links_graph[source_url])
                        if outgoing_count > 0:
                            # –ü–µ—Ä–µ–¥–∞–µ–º —á–∞—Å—Ç—å PageRank –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
                            incoming_rank += pagerank[source_url] / outgoing_count
                
                new_pagerank[target_url] = base_rank + damping_factor * incoming_rank
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
            max_diff = max(abs(new_pagerank[url] - pagerank[url]) for url in urls)
            pagerank = new_pagerank
            
            if (iteration + 1) % 5 == 0:
                self.add_log(f"PageRank –∏—Ç–µ—Ä–∞—Ü–∏—è {iteration + 1}/{iterations}, —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å: {max_diff:.6f}", "info")
            
            # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
            if max_diff < 1e-6:
                self.add_log(f"PageRank —Å–æ—à–µ–ª—Å—è –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration + 1}", "success")
                break
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        total_rank = sum(pagerank.values())
        if total_rank > 0:
            for url in urls:
                pagerank[url] = (pagerank[url] / total_rank) * n_pages
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü
        for url, rank in pagerank.items():
            self.pages_data[url].page_rank = rank
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
        sorted_pages = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)
        
        self.add_log(f"PageRank —Ä–∞—Å—Å—á–∏—Ç–∞–Ω! –¢–æ–ø-5 —Å—Ç—Ä–∞–Ω–∏—Ü:", "success")
        for i, (url, rank) in enumerate(sorted_pages[:5]):
            short_url = self.get_short_url(url)
            incoming_links = self.pages_data[url].internal_links_count
            self.add_log(f"  {i+1}. {short_url}: {rank:.4f} ({incoming_links} –≤—Ö–æ–¥—è—â–∏—Ö)", "info")
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥—Ä–∞—Ñ–µ
        total_links = sum(len(links) for links in self.internal_links_graph.values())
        self.add_log(f"üìä –ì—Ä–∞—Ñ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {len(self.pages_data)} —Å—Ç—Ä–∞–Ω–∏—Ü, {total_links} —Å–≤—è–∑–µ–π", "info")
        
        return pagerank

    def get_short_url(self, url: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫—É—é –≤–µ—Ä—Å–∏—é URL –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        
        if not path_parts:
            return "–ì–ª–∞–≤–Ω–∞—è"
        elif len(path_parts) == 1:
            return path_parts[0]
        else:
            return f".../{path_parts[-1]}"

    async def analyze_page(self, session: aiohttp.ClientSession, url: str, html: str, response) -> PageSEOData:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å–±–æ—Ä SEO-–¥–∞–Ω–Ω—ã—Ö"""
        start_time = time.time()
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        page_data = PageSEOData(
            url=url,
            status_code=response.status,
            content_type=response.headers.get('content-type', ''),
            response_time=time.time() - start_time
        )

        try:
            # Title –∏ Meta Description
            if soup.title:
                page_data.title = soup.title.string.strip() if soup.title.string else ""
            
            meta_desc = soup.find('meta', {'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                page_data.meta_description = meta_desc['content'].strip()

            # –ó–∞–≥–æ–ª–æ–≤–∫–∏
            page_data.h1 = [h1.get_text(strip=True) for h1 in soup.find_all('h1')]
            page_data.h2 = [h2.get_text(strip=True) for h2 in soup.find_all('h2')]

            # Canonical –∏ Robots meta
            canonical = soup.find('link', {'rel': 'canonical'})
            page_data.canonical = canonical['href'] if canonical else ""
            
            robots_meta = soup.find('meta', {'name': 'robots'})
            page_data.robots_meta = robots_meta['content'] if robots_meta else ""

            # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            text_elements = []
            for tag in ['p', 'div', 'span', 'article', 'section', 'main']:
                text_elements.extend([elem.get_text(strip=True) for elem in soup.find_all(tag)])
            text_content = ' '.join(text_elements)
            page_data.word_count = len(re.findall(r'\w+', text_content))
            page_data.content_length = len(html)

            # –•–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            content_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
            page_data.content_hash = content_hash
            if content_hash in self.content_hashes and self.content_hashes[content_hash]:
                page_data.duplicate_content = True
            self.content_hashes[content_hash].append(url)

            # –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if self.config['check_images']:
                for img in soup.find_all('img'):
                    img_data = {
                        'src': img.get('src', ''),
                        'alt': img.get('alt', ''),
                        'title': img.get('title', ''),
                    }
                    if img_data['src']:
                        img_data['src'] = urljoin(url, img_data['src'])
                        page_data.images.append(img_data)

            # Schema.org
            if self.config['check_schema']:
                schema_tags = soup.find_all('script', {'type': 'application/ld+json'})
                page_data.schema_org = [tag.string for tag in schema_tags if tag.string]

            # Open Graph
            if self.config['check_social_tags']:
                for og in soup.find_all('meta', attrs={'property': re.compile('^og:')}):
                    page_data.open_graph[og['property']] = og.get('content', '')

            # Twitter Cards
            if self.config['check_social_tags']:
                for twitter in soup.find_all('meta', attrs={'name': re.compile('^twitter:')}):
                    page_data.twitter_cards[twitter['name']] = twitter.get('content', '')

            # Hreflang
            if self.config['check_hreflang']:
                for hreflang in soup.find_all('link', {'rel': 'alternate', 'hreflang': True}):
                    page_data.hreflang[hreflang['hreflang']] = hreflang['href']

            # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫
            for link in soup.find_all('a', href=True):
                href = link['href'].strip()
                if not href or href.startswith(('#', 'mailto:', 'tel:', 'javascript:', 'data:')):
                    continue
                
                full_url = urljoin(url, href)
                normalized_url = self.normalize_url(full_url)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π
                if self.is_main_domain_only(normalized_url):
                    page_data.outlinks.append(normalized_url)
                else:
                    page_data.inlinks.append(normalized_url)

        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {url}: {str(e)}")
            self.add_log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {url}", "error")

        return page_data

    async def fetch_robots_txt(self, session):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ robots.txt"""
        robots_url = urljoin(self.start_url, '/robots.txt')
        try:
            async with session.get(robots_url) as response:
                if response.status == 200:
                    content = await response.text()
                    self.robots_parser = RobotFileParser(robots_url)
                    self.robots_parser.set_url(robots_url)
                    self.robots_parser.parse(content.splitlines())
                    self.add_log("robots.txt –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ", "success")
                else:
                    self.log_error(f"robots.txt –Ω–µ –Ω–∞–π–¥–µ–Ω (—Å—Ç–∞—Ç—É—Å {response.status})")
                    self.add_log("robots.txt –Ω–µ –Ω–∞–π–¥–µ–Ω", "warning")
        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ robots.txt: {e}")
            self.add_log("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ robots.txt", "warning")
            self.robots_parser = None

    def can_fetch(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è URL –≤ robots.txt"""
        if not self.config['follow_robots_txt'] or not self.robots_parser:
            return True
        return self.robots_parser.can_fetch(self.headers['User-Agent'], url)

    def generate_seo_table(self) -> Table:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å SEO-–¥–∞–Ω–Ω—ã–º–∏"""
        table = Table(box=box.ROUNDED, title="üîç SEO –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
        table.add_column("URL", style="cyan", width=40)
        table.add_column("–ó–∞–≥–æ–ª–æ–≤–æ–∫", width=30)
        table.add_column("–°—Ç–∞—Ç—É—Å", justify="center")
        table.add_column("H1", width=20)
        table.add_column("–û–ø–∏—Å–∞–Ω–∏–µ", width=30)
        table.add_column("–ü—Ä–æ–±–ª–µ–º—ã", style="red")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        for url, data in list(self.pages_data.items())[-5:]:
            issues = []
            
            if not data.title:
                issues.append("–ù–µ—Ç Title")
            elif len(data.title) > 60:
                issues.append("–î–ª–∏–Ω–Ω—ã–π Title")
                
            if not data.meta_description:
                issues.append("–ù–µ—Ç Meta")
            elif len(data.meta_description) > 160:
                issues.append("–î–ª–∏–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")
                
            if not data.h1:
                issues.append("–ù–µ—Ç H1")
            elif len(data.h1) > 1:
                issues.append("–ú–Ω–æ–≥–æ H1")

            if data.duplicate_content:
                issues.append("–î—É–±–ª–∏–∫–∞—Ç")

            if data.word_count < self.config['min_word_count']:
                issues.append("–ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞")

            status_color = self.get_status_color(data.status_code)
            
            table.add_row(
                Text(self.get_short_url(url), overflow="ellipsis"),
                Text(data.title[:30] + "..." if data.title else "", overflow="ellipsis"),
                f"[{status_color}]{data.status_code}[/{status_color}]",
                Text(data.h1[0][:20] + "..." if data.h1 else "", overflow="ellipsis"),
                Text(data.meta_description[:30] + "..." if data.meta_description else "", overflow="ellipsis"),
                ", ".join(issues) if issues else "[green]OK[/green]"
            )
        return table

    def generate_stats_table(self) -> Table:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        table = Table(box=box.ROUNDED, title="üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
        table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
        table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", justify="right")

        total_pages = len(self.pages_data)
        issues_count = sum(1 for data in self.pages_data.values() if not data.title or not data.meta_description or not data.h1)
        duplicate_count = sum(1 for data in self.pages_data.values() if data.duplicate_content)
        
        table.add_row("–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü", str(total_pages))
        table.add_row("–û—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω", f"[blue]{self.main_domain}[/blue]")
        table.add_row("–°—Ç—Ä–∞–Ω–∏—Ü —Å –æ—à–∏–±–∫–∞–º–∏", f"[red]{issues_count}[/red]")
        table.add_row("404 –æ—à–∏–±–∫–∏", f"[red]{len(self.not_found_urls)}[/red]")
        table.add_row("–†–µ–¥–∏—Ä–µ–∫—Ç—ã", f"[yellow]{len(self.redirects)}[/yellow]")
        table.add_row("–î—É–±–ª–∏–∫–∞—Ç—ã", f"[yellow]{duplicate_count}[/yellow]")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–¥–∞–º –æ—Ç–≤–µ—Ç–∞
        for status, count in sorted(self.status_counts.items()):
            color = self.get_status_color(status)
            table.add_row(f"–°—Ç–∞—Ç—É—Å {status}", f"[{color}]{count}[/{color}]")
        
        table.add_row("–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä", f"{sum(d.content_length for d in self.pages_data.values()) // (total_pages or 1)} –±–∞–π")
        table.add_row("–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞", f"{sum(d.response_time for d in self.pages_data.values()) / (total_pages or 1):.2f} —Å–µ–∫")
        
        return table

    def generate_pagerank_table(self) -> Table:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å —Ç–æ–ø-—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –ø–æ PageRank"""
        table = Table(box=box.ROUNDED, title="üèÜ –¢–æ–ø-10 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ PageRank")
        table.add_column("–†–∞–Ω–≥", style="cyan", justify="center")
        table.add_column("URL", style="blue", width=40)
        table.add_column("PageRank", justify="right")
        table.add_column("–í—Ö–æ–¥—è—â–∏–µ", justify="center")
        table.add_column("–ò—Å—Ö–æ–¥—è—â–∏–µ", justify="center")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ PageRank
        sorted_pages = sorted(
            self.pages_data.items(), 
            key=lambda x: x[1].page_rank, 
            reverse=True
        )[:10]

        for i, (url, data) in enumerate(sorted_pages, 1):
            short_url = self.get_short_url(url)
            
            table.add_row(
                str(i),
                Text(short_url, overflow="ellipsis"),
                f"{data.page_rank:.4f}",
                str(data.internal_links_count),
                str(len(data.outlinks))
            )
        
        return table

    def generate_display(self) -> Layout:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        layout = Layout()
        
        layout.split_column(
            Layout(name="header", size=7),
            Layout(name="progress", size=4),
            Layout(name="main", size=18),
            Layout(name="footer", size=3),
            Layout(name="log", size=10)
        )

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ª—è–≥—É—à–∫–æ–π
        self.current_frame = (self.current_frame + 1) % len(self.frog_frames)
        header_content = Panel(
            f"{self.frog_frames[self.current_frame]}\n"
            f"üåê SEO –ê–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞: {self.main_domain}\n"
            f"üìë –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {self.total_scanned}",
            title="SEO Frog Scanner",
            style="bold green",
            border_style="green"
        )
        layout["header"].update(header_content)

        # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        main_layout = Layout()
        main_layout.split_row(
            Layout(
                Panel(
                    self.generate_stats_table(),
                    title="üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
                    border_style="blue"
                ),
                size=40
            ),
            Layout(
                Panel(
                    self.generate_seo_table(),
                    title="üîç –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                    border_style="cyan"
                ),
                size=60
            )
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É PageRank, –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if self.pages_data and any(data.page_rank > 0 for data in self.pages_data.values()):
            pagerank_layout = Layout()
            pagerank_layout.split_column(
                main_layout,
                Layout(
                    Panel(
                        self.generate_pagerank_table(),
                        title="üèÜ PageRank –ê–Ω–∞–ª–∏–∑",
                        border_style="green"
                    ),
                    size=12
                )
            )
            layout["main"].update(pagerank_layout)
        else:
            layout["main"].update(main_layout)

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        if self.progress_data['start_time']:
            elapsed_time = time.time() - self.progress_data['start_time']
            if self.progress_data['scanned'] > 0:
                avg_time_per_url = elapsed_time / self.progress_data['scanned']
                remaining_urls = max(0, self.estimated_total_urls - self.progress_data['scanned'])
                estimated_remaining = remaining_urls * avg_time_per_url
                
                progress_percent = min(100, (self.progress_data['scanned'] / max(1, self.estimated_total_urls)) * 100)
                
                progress_content = Panel(
                    f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {self.progress_data['scanned']}/{self.estimated_total_urls} ({progress_percent:.1f}%)\n"
                    f"‚è±Ô∏è –ü—Ä–æ—à–ª–æ –≤—Ä–µ–º–µ–Ω–∏: {elapsed_time:.0f}—Å | "
                    f"‚è≥ –û—Å—Ç–∞–ª–æ—Å—å: {estimated_remaining:.0f}—Å | "
                    f"üìà –ù–∞–π–¥–µ–Ω–æ: {self.progress_data['found']} | "
                    f"‚ùå –û—à–∏–±–æ–∫: {self.progress_data['errors']}",
                    title="üîÑ –°—Ç–∞—Ç—É—Å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è",
                    border_style="green"
                )
            else:
                progress_content = Panel(
                    "üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...",
                    title="üîÑ –°—Ç–∞—Ç—É—Å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è",
                    border_style="yellow"
                )
        else:
            progress_content = Panel(
                "‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...",
                title="üîÑ –°—Ç–∞—Ç—É—Å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è",
                border_style="blue"
            )
        layout["progress"].update(progress_content)

        # –§—É—Ç–µ—Ä
        footer_content = Panel(
            f"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {self.get_short_url(self.current_url) if self.current_url else '–û–∂–∏–¥–∞–Ω–∏–µ...'}",
            style="bold blue",
            border_style="blue"
        )
        layout["footer"].update(footer_content)

        # –õ–æ–≥–∏
        recent_logs = self.get_recent_logs()
        log_content = Panel(
            "\n".join(recent_logs),
            title="üìù –ü–æ—Å–ª–µ–¥–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è",
            border_style="yellow"
        )
        layout["log"].update(log_content)

        return layout
    
    async def scan_site(self, session: aiohttp.ClientSession, live: Live):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∞–π—Ç–∞"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.progress_data['start_time'] = time.time()
        self.estimate_total_urls()
        
        async def process_url(url: str, depth: int = 0, source_url: str = None):
            await asyncio.sleep(0.3)  # –£–º–µ–Ω—å—à–µ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            
            if depth > self.config['max_depth'] or url in self.visited_urls:
                return

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
            normalized_url = self.normalize_url(url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É –¥–æ–º–µ–Ω—É
            if not self.is_main_domain_only(normalized_url):
                return
            
            if normalized_url in self.visited_urls:
                return

            self.current_url = normalized_url
            self.visited_urls.add(normalized_url)
            
            try:
                async with session.get(normalized_url, headers=self.headers, timeout=30, allow_redirects=True) as response:
                    self.status_counts[response.status] += 1
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
                    if response.history:
                        redirect_chain = ' -> '.join([str(r.status) for r in response.history] + [str(response.status)])
                        self.add_log(f"–†–µ–¥–∏—Ä–µ–∫—Ç: {self.get_short_url(normalized_url)} ({redirect_chain})", "warning")
                        self.redirects[normalized_url] = {
                            'from': normalized_url,
                            'to': str(response.url),
                            'chain': redirect_chain
                        }

                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
                    if response.status == 404:
                        error_msg = f"404: {normalized_url} (–∏—Å—Ç–æ—á–Ω–∏–∫: {source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'})"
                        self.log_error(error_msg)
                        self.add_log(f"404: {self.get_short_url(normalized_url)}", "error")
                        self.not_found_urls.append({'url': normalized_url, 'source': source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'})
                        self.error_sources[normalized_url].append(source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞')
                        self.estimate_total_urls()  # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                        return
                    elif response.status >= 400:
                        error_msg = f"–û—à–∏–±–∫–∞ {response.status}: {normalized_url} (–∏—Å—Ç–æ—á–Ω–∏–∫: {source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'})"
                        self.log_error(error_msg)
                        self.add_log(f"–û—à–∏–±–∫–∞ {response.status}: {self.get_short_url(normalized_url)}", "error")
                        self.error_urls.append({
                            'url': normalized_url, 
                            'status': response.status, 
                            'source': source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'
                        })
                        self.error_sources[normalized_url].append(source_url or '–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞')
                        self.estimate_total_urls()  # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                        return
                    else:
                        self.add_log(f"–°–∫–∞–Ω–∏—Ä—É–µ–º: {self.get_short_url(normalized_url)}", "info")

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º content-type
                    content_type = response.headers.get('content-type', '').lower()
                    if 'text/html' not in content_type:
                        return

                    html = await response.text()
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    page_data = await self.analyze_page(session, normalized_url, html, response)
                    self.pages_data[normalized_url] = page_data
                    self.total_scanned += 1
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –≥—Ä–∞—Ñ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    self.update_internal_links_for_page(normalized_url, page_data)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                    self.estimate_total_urls()
                    await self.auto_save_check()
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    live.update(self.generate_display())
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
                    soup = BeautifulSoup(html, 'html.parser')
                    links = set()
                    
                    # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
                    for link in soup.find_all('a', href=True):
                        href = link.get('href', '').strip()
                        if href and not href.startswith(('#', 'mailto:', 'tel:', 'javascript:', 'data:')):
                            full_url = urljoin(normalized_url, href)
                            normalized_link = self.normalize_url(full_url)
                            if (self.is_main_domain_only(normalized_link) and 
                                self.can_fetch(normalized_link) and 
                                normalized_link not in self.visited_urls):
                                links.add(normalized_link)
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–µ–±–æ–ª—å—à–∏–º–∏ –≥—Ä—É–ø–ø–∞–º–∏
                    tasks = []
                    for next_url in links:
                        tasks.append(process_url(next_url, depth + 1, normalized_url))
                    
                    if tasks:
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                        for i in range(0, len(tasks), 3):
                            batch = tasks[i:i+3]
                            await asyncio.gather(*batch, return_exceptions=True)
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
                            self.estimate_total_urls()
                            live.update(self.generate_display())
                        
            except asyncio.TimeoutError:
                error_msg = f"–¢–∞–π–º–∞—É—Ç: {normalized_url}"
                self.log_error(error_msg)
                self.add_log(f"–¢–∞–π–º–∞—É—Ç: {self.get_short_url(normalized_url)}", "error")
                self.estimate_total_urls()  # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {normalized_url}: {str(e)}"
                self.log_error(error_msg)
                self.add_log(f"–û—à–∏–±–∫–∞: {self.get_short_url(normalized_url)}", "error")
                self.estimate_total_urls()  # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ URL
        await process_url(self.start_url)

    async def export_results(self, is_autosave: bool = False):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
        if is_autosave:
            self.add_log(f"üîÑ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ({len(self.pages_data)} —Å—Ç—Ä–∞–Ω–∏—Ü)...", "info")
        else:
            self.add_log("–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...", "info")
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç—á–µ—Ç
        main_data = []
        for url, data in self.pages_data.items():
            main_data.append({
                'URL': url,
                '–°—Ç–∞—Ç—É—Å': data.status_code,
                '–ó–∞–≥–æ–ª–æ–≤–æ–∫': data.title,
                '–ú–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ': data.meta_description,
                'H1': ' | '.join(data.h1),
                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤': data.word_count,
                '–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (—Å–µ–∫)': f"{data.response_time:.2f}",
                '–†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±–∞–π—Ç)': data.content_length,
                '–î—É–±–ª–∏–∫–∞—Ç': '–î–∞' if data.duplicate_content else '–ù–µ—Ç',
                'PageRank': f"{data.page_rank:.6f}",
                '–í—Ö–æ–¥—è—â–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏': data.internal_links_count,
                '–ò—Å—Ö–æ–¥—è—â–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏': len(data.outlinks),
                '–í–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏': len(data.inlinks),
                'Canonical': data.canonical,
                'Robots Meta': data.robots_meta,
                '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π': len(data.images),
                'Schema.org': len(data.schema_org),
                'Open Graph': len(data.open_graph),
                'Twitter Cards': len(data.twitter_cards),
                'Hreflang': len(data.hreflang),
                '–ü—Ä–æ–±–ª–µ–º—ã': self.get_page_issues(data)
            })

        if main_data:
            df_main = pd.DataFrame(main_data)
            
            # –î–ª—è –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            if is_autosave:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                filename = f'seo_–æ—Ç—á–µ—Ç_–æ—Å–Ω–æ–≤–Ω–æ–π_autosave_{timestamp}.xlsx'
            else:
                filename = 'seo_–æ—Ç—á–µ—Ç_–æ—Å–Ω–æ–≤–Ω–æ–π.xlsx'
            
            df_main.to_excel(filename, index=False)

        # –û—Ç—á–µ—Ç –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        images_data = []
        for url, data in self.pages_data.items():
            for img in data.images:
                images_data.append({
                    'URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã': url,
                    'URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è': img['src'],
                    'Alt —Ç–µ–∫—Å—Ç': img['alt'],
                    'Title': img['title'],
                    '–ï—Å—Ç—å Alt': '–î–∞' if img['alt'] else '–ù–µ—Ç'
                })

        if images_data:
            df_images = pd.DataFrame(images_data)
            if is_autosave:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                filename = f'seo_–æ—Ç—á–µ—Ç_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è_autosave_{timestamp}.xlsx'
            else:
                filename = 'seo_–æ—Ç—á–µ—Ç_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.xlsx'
            df_images.to_excel(filename, index=False)

        # –û—Ç—á–µ—Ç –ø–æ –¥—É–±–ª–∏–∫–∞—Ç–∞–º
        duplicates_data = []
        duplicate_groups = {}
        group_id = 1
        
        for content_hash, urls in self.content_hashes.items():
            if len(urls) > 1:
                duplicate_groups[content_hash] = group_id
                for url in urls:
                    duplicates_data.append({
                        'URL': url,
                        '–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤': group_id,
                        '–•–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞': content_hash,
                        '–ó–∞–≥–æ–ª–æ–≤–æ–∫': self.pages_data[url].title if url in self.pages_data else '',
                        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤': self.pages_data[url].word_count if url in self.pages_data else 0,
                        '–í—Å–µ–≥–æ –≤ –≥—Ä—É–ø–ø–µ': len(urls)
                    })
                group_id += 1

        if duplicates_data:
            df_duplicates = pd.DataFrame(duplicates_data)
            df_duplicates = df_duplicates.sort_values(['–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤', 'URL'])
            if is_autosave:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                filename = f'seo_–æ—Ç—á–µ—Ç_–¥—É–±–ª–∏–∫–∞—Ç—ã_autosave_{timestamp}.xlsx'
            else:
                filename = 'seo_–æ—Ç—á–µ—Ç_–¥—É–±–ª–∏–∫–∞—Ç—ã.xlsx'
            df_duplicates.to_excel(filename, index=False)

        # –û—Ç—á–µ—Ç –ø–æ PageRank
        if self.config['calculate_pagerank'] and self.pages_data:
            pagerank_data = []
            sorted_pages = sorted(
                self.pages_data.items(), 
                key=lambda x: x[1].page_rank, 
                reverse=True
            )
            
            for rank_position, (url, data) in enumerate(sorted_pages, 1):
                pagerank_data.append({
                    '–ü–æ–∑–∏—Ü–∏—è': rank_position,
                    'URL': url,
                    'PageRank': data.page_rank,
                    '–í—Ö–æ–¥—è—â–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏': data.internal_links_count,
                    '–ò—Å—Ö–æ–¥—è—â–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏': len(data.outlinks),
                    '–í–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏': len(data.inlinks),
                    '–ó–∞–≥–æ–ª–æ–≤–æ–∫': data.title,
                    '–°—Ç–∞—Ç—É—Å': data.status_code,
                    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤': data.word_count,
                    '–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (—Å–µ–∫)': f"{data.response_time:.2f}"
                })
            
            if pagerank_data:
                df_pagerank = pd.DataFrame(pagerank_data)
                if is_autosave:
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    filename = f'seo_–æ—Ç—á–µ—Ç_pagerank_autosave_{timestamp}.xlsx'
                else:
                    filename = 'seo_–æ—Ç—á–µ—Ç_pagerank.xlsx'
                df_pagerank.to_excel(filename, index=False)

        # –û—Ç—á–µ—Ç –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å—Å—ã–ª–∫–∞–º
        internal_links_data = []
        for source_url, target_urls in self.internal_links_graph.items():
            for target_url in target_urls:
                internal_links_data.append({
                    '–ò—Å—Ç–æ—á–Ω–∏–∫': source_url,
                    '–¶–µ–ª—å': target_url,
                    'PageRank –∏—Å—Ç–æ—á–Ω–∏–∫–∞': self.pages_data[source_url].page_rank if source_url in self.pages_data else 0,
                    'PageRank —Ü–µ–ª–∏': self.pages_data[target_url].page_rank if target_url in self.pages_data else 0,
                    '–ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞': self.pages_data[source_url].title if source_url in self.pages_data else '',
                    '–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ü–µ–ª–∏': self.pages_data[target_url].title if target_url in self.pages_data else ''
                })

        if internal_links_data:
            df_internal_links = pd.DataFrame(internal_links_data)
            df_internal_links = df_internal_links.sort_values('PageRank —Ü–µ–ª–∏', ascending=False)
            if is_autosave:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                filename = f'seo_–æ—Ç—á–µ—Ç_–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ_—Å—Å—ã–ª–∫–∏_autosave_{timestamp}.xlsx'
            else:
                filename = 'seo_–æ—Ç—á–µ—Ç_–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ_—Å—Å—ã–ª–∫–∏.xlsx'
            df_internal_links.to_excel(filename, index=False)

        # –û—Ç—á–µ—Ç –ø–æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º
        if self.redirects:
            redirects_data = []
            for redirect_data in self.redirects.values():
                redirects_data.append({
                    '–° URL': redirect_data['from'],
                    '–ù–∞ URL': redirect_data['to'],
                    '–¶–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤': redirect_data['chain']
                })
            df_redirects = pd.DataFrame(redirects_data)
            if is_autosave:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                filename = f'seo_–æ—Ç—á–µ—Ç_—Ä–µ–¥–∏—Ä–µ–∫—Ç—ã_autosave_{timestamp}.xlsx'
            else:
                filename = 'seo_–æ—Ç—á–µ—Ç_—Ä–µ–¥–∏—Ä–µ–∫—Ç—ã.xlsx'
            df_redirects.to_excel(filename, index=False)

        # –û—Ç—á–µ—Ç –ø–æ –æ—à–∏–±–∫–∞–º
        if self.error_urls or self.not_found_urls:
            errors_data = []
            
            # 404 –æ—à–∏–±–∫–∏
            for error in self.not_found_urls:
                sources = self.error_sources.get(error['url'], [])
                errors_data.append({
                    'URL': error['url'],
                    '–¢–∏–ø –æ—à–∏–±–∫–∏': '404 –ù–µ –Ω–∞–π–¥–µ–Ω–æ',
                    '–û—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫': error['source'],
                    '–í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏': ' | '.join(set(sources)) if sources else error['source'],
                    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤': len(set(sources)) if sources else 1
                })
            
            # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏
            for error in self.error_urls:
                sources = self.error_sources.get(error['url'], [])
                errors_data.append({
                    'URL': error['url'],
                    '–¢–∏–ø –æ—à–∏–±–∫–∏': f"–û—à–∏–±–∫–∞ {error.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}",
                    '–û—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫': error['source'],
                    '–í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏': ' | '.join(set(sources)) if sources else error['source'],
                    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤': len(set(sources)) if sources else 1
                })

            if errors_data:
                df_errors = pd.DataFrame(errors_data)
                df_errors = df_errors.sort_values('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤', ascending=False)
                if is_autosave:
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    filename = f'seo_–æ—Ç—á–µ—Ç_–æ—à–∏–±–∫–∏_autosave_{timestamp}.xlsx'
                else:
                    filename = 'seo_–æ—Ç—á–µ—Ç_–æ—à–∏–±–∫–∏.xlsx'
                df_errors.to_excel(filename, index=False)

        # –≠–∫—Å–ø–æ—Ä—Ç –≤ XML (Sitemap)
        if not is_autosave:
            self.export_to_xml()

        # –≠–∫—Å–ø–æ—Ä—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞
        if not is_autosave:
            self.export_site_structure(is_autosave=False)

    def export_to_xml(self):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ XML sitemap —Ñ–æ—Ä–º–∞—Ç"""
        urlset = ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ PageRank –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
        sorted_pages = sorted(
            self.pages_data.items(), 
            key=lambda x: x[1].page_rank, 
            reverse=True
        )
        
        for url, data in sorted_pages:
            if data.status_code == 200:  # –¢–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                url_elem = ET.SubElement(urlset, "url")
                ET.SubElement(url_elem, "loc").text = url
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ PageRank (–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0.1-1.0)
                max_rank = max(d.page_rank for d in self.pages_data.values())
                priority = max(0.1, min(1.0, data.page_rank / max_rank))
                ET.SubElement(url_elem, "priority").text = f"{priority:.1f}"
                
                # –ß–∞—Å—Ç–æ—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Ä–æ–≤–Ω—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
                depth = len([p for p in urlparse(url).path.split('/') if p])
                if depth <= 1:
                    changefreq = "daily"
                elif depth <= 2:
                    changefreq = "weekly"
                else:
                    changefreq = "monthly"
                ET.SubElement(url_elem, "changefreq").text = changefreq

        tree = ET.ElementTree(urlset)
        tree.write("sitemap.xml", encoding='utf-8', xml_declaration=True)

    def export_site_structure(self, is_autosave: bool = False):
        """–≠–∫—Å–ø–æ—Ä—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞"""
        structure_data = []
        
        for url, data in self.pages_data.items():
            parsed = urlparse(url)
            path_parts = [p for p in parsed.path.split('/') if p]
            
            structure_data.append({
                'URL': url,
                '–£—Ä–æ–≤–µ–Ω—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏': len(path_parts),
                '–ü—É—Ç—å': parsed.path,
                '–†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞': '/'.join(parsed.path.split('/')[:-1]) if len(path_parts) > 0 else '/',
                'PageRank': data.page_rank,
                '–í—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏': data.internal_links_count,
                '–ó–∞–≥–æ–ª–æ–≤–æ–∫': data.title,
                '–°—Ç–∞—Ç—É—Å': data.status_code
            })
        
        if structure_data:
            df_structure = pd.DataFrame(structure_data)
            df_structure = df_structure.sort_values(['–£—Ä–æ–≤–µ–Ω—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏', 'PageRank'], ascending=[True, False])
            if is_autosave:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                filename = f'seo_–æ—Ç—á–µ—Ç_—Å—Ç—Ä—É–∫—Ç—É—Ä–∞_—Å–∞–π—Ç–∞_autosave_{timestamp}.xlsx'
            else:
                filename = 'seo_–æ—Ç—á–µ—Ç_—Å—Ç—Ä—É–∫—Ç—É—Ä–∞_—Å–∞–π—Ç–∞.xlsx'
            df_structure.to_excel(filename, index=False)

    def get_page_issues(self, data: PageSEOData) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        issues = []
        
        # SEO –ø—Ä–æ–±–ª–µ–º—ã
        if not data.title:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç Title")
        elif len(data.title) > 60:
            issues.append("Title —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (>60)")
        elif len(data.title) < 30:
            issues.append("Title —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (<30)")
        
        if not data.meta_description:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç Meta Description")
        elif len(data.meta_description) > 160:
            issues.append("Meta Description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (>160)")
        elif len(data.meta_description) < 120:
            issues.append("Meta Description —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (<120)")
        
        if not data.h1:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç H1")
        elif len(data.h1) > 1:
            issues.append("–ù–µ—Å–∫–æ–ª—å–∫–æ H1 —Ç–µ–≥–æ–≤")
        
        # –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        if data.duplicate_content:
            issues.append("–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç")
        
        if data.word_count < self.config['min_word_count']:
            issues.append(f"–ú–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (<{self.config['min_word_count']} —Å–ª–æ–≤)")
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        if data.response_time > self.config['max_response_time']:
            issues.append("–ú–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç")
        
        if data.page_rank < 0.0001:  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π PageRank
            issues.append("–ù–∏–∑–∫–∏–π PageRank")
        
        # –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        images_without_alt = sum(1 for img in data.images if not img.get('alt'))
        if images_without_alt > 0:
            issues.append(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ Alt ({images_without_alt})")
        
        return " | ".join(issues) if issues else "–ù–µ—Ç –ø—Ä–æ–±–ª–µ–º"

    def get_status_color(self, status_code: int) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–≤–µ—Ç –¥–ª—è —Å—Ç–∞—Ç—É—Å –∫–æ–¥–∞"""
        if status_code < 300:
            return "green"
        elif status_code < 400:
            return "yellow"
        elif status_code < 500:
            return "red"
        else:
            return "red bold"

    def add_log(self, message: str, level: str = "info"):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –ª–æ–≥"""
        timestamp = time.strftime("%H:%M:%S")
        color = {
            "info": "blue",
            "warning": "yellow",
            "error": "red",
            "success": "green"
        }.get(level, "white")
        
        log_entry = f"[{color}]{timestamp} | {message}[/{color}]"
        self.logs.append(log_entry)
        
        if len(self.logs) > self.max_logs:
            self.logs.pop(0)

    def get_recent_logs(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ª–æ–≥–æ–≤"""
        return self.logs

    def log_error(self, message: str):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –æ—à–∏–±–∫—É –≤ –ª–æ–≥-—Ñ–∞–π–ª"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        with open(self.error_log_file, 'a', encoding='utf-8') as f:
            f.write(f"[{timestamp}] {message}\n")

    async def auto_save_check(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        if len(self.pages_data) - self.last_save_count >= self.save_interval:
            self.add_log(f"üîÑ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ {self.save_interval} —Å—Ç—Ä–∞–Ω–∏—Ü...", "info")
            await self.export_results(is_autosave=True)
            self.last_save_count = len(self.pages_data)
            self.add_log(f"‚úÖ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ ({len(self.pages_data)} —Å—Ç—Ä–∞–Ω–∏—Ü)", "success")
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            self.cleanup_old_autosaves()

    def estimate_total_urls(self):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        if not self.pages_data:
            self.estimated_total_urls = 100  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
            return
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        all_links = set()
        for page_data in self.pages_data.values():
            all_links.update(page_data.outlinks)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL
        all_links.update(self.visited_urls)
        
        # –û—Ü–µ–Ω–∫–∞: –µ—Å–ª–∏ –º—ã –Ω–∞—à–ª–∏ –º–Ω–æ–≥–æ —Å—Å—ã–ª–æ–∫, –Ω–æ –ø–æ—Å–µ—Ç–∏–ª–∏ –º–∞–ª–æ, –∑–Ω–∞—á–∏—Ç –∏—Ö –±–æ–ª—å—à–µ
        if len(all_links) > len(self.visited_urls) * 2:
            self.estimated_total_urls = max(self.estimated_total_urls, len(all_links) * 1.5)
        else:
            # –ï—Å–ª–∏ —Å—Å—ã–ª–æ–∫ –º–∞–ª–æ, –≤–æ–∑–º–æ–∂–Ω–æ –º—ã –±–ª–∏–∑–∫–∏ –∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é
            self.estimated_total_urls = max(self.estimated_total_urls, len(all_links) * 1.2)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        self.progress_data['scanned'] = len(self.visited_urls)
        self.progress_data['found'] = len(self.pages_data)
        self.progress_data['errors'] = len(self.error_urls) + len(self.not_found_urls)

    def cleanup_old_autosaves(self):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3"""
        import glob
        import os
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        patterns = [
            'seo_–æ—Ç—á–µ—Ç_–æ—Å–Ω–æ–≤–Ω–æ–π_autosave_*.xlsx',
            'seo_–æ—Ç—á–µ—Ç_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è_autosave_*.xlsx',
            'seo_–æ—Ç—á–µ—Ç_–¥—É–±–ª–∏–∫–∞—Ç—ã_autosave_*.xlsx',
            'seo_–æ—Ç—á–µ—Ç_pagerank_autosave_*.xlsx',
            'seo_–æ—Ç—á–µ—Ç_–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ_—Å—Å—ã–ª–∫–∏_autosave_*.xlsx',
            'seo_–æ—Ç—á–µ—Ç_—Ä–µ–¥–∏—Ä–µ–∫—Ç—ã_autosave_*.xlsx',
            'seo_–æ—Ç—á–µ—Ç_–æ—à–∏–±–∫–∏_autosave_*.xlsx',
            'seo_–æ—Ç—á–µ—Ç_—Å—Ç—Ä—É–∫—Ç—É—Ä–∞_—Å–∞–π—Ç–∞_autosave_*.xlsx'
        ]
        
        for pattern in patterns:
            files = glob.glob(pattern)
            if len(files) > 3:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è –∏ —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ
                files.sort(key=os.path.getctime, reverse=True)
                for old_file in files[3:]:
                    try:
                        os.remove(old_file)
                        self.add_log(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {old_file}", "info")
                    except Exception as e:
                        self.add_log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {old_file}: {e}", "error")

    async def run(self):
        """–ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.console.clear()
        self.add_log(f"–ù–∞—á–∞–ª–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–º–µ–Ω–∞: {self.main_domain}", "info")
        self.add_log(f"–†–µ–∂–∏–º: —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω {'‚úì' if self.config['main_domain_only'] else '‚úó'}", "info")
        
        timeout = aiohttp.ClientTimeout(total=60, connect=10)
        connector = aiohttp.TCPConnector(
            limit=10, 
            force_close=True, 
            enable_cleanup_closed=True,
            limit_per_host=5
        )
        
        try:
            with Live(self.generate_display(), refresh_per_second=2, screen=True) as live:
                async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º robots.txt
                    await self.fetch_robots_txt(session)
                    
                    # –°–∫–∞–Ω–∏—Ä—É–µ–º —Å–∞–π—Ç
                    await self.scan_site(session, live)

                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º PageRank –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
                    if self.config['calculate_pagerank'] and self.pages_data:
                        self.calculate_internal_pagerank()

                    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    await self.export_results()
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            total_time = time.time() - self.progress_data['start_time'] if self.progress_data['start_time'] else 0
            self.console.print("\n[green]‚úÖ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ![/green]")
            self.console.print(f"[blue]üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {self.total_scanned}[/blue]")
            self.console.print(f"[blue]üåê –û—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω: {self.main_domain}[/blue]")
            self.console.print(f"[blue]‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.0f} —Å–µ–∫—É–Ω–¥[/blue]")
            self.console.print(f"[blue]üìà –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {self.total_scanned / max(1, total_time):.1f} —Å—Ç—Ä–∞–Ω–∏—Ü/—Å–µ–∫[/blue]")
            
            if self.pages_data:
                avg_pagerank = sum(d.page_rank for d in self.pages_data.values()) / len(self.pages_data)
                max_pagerank = max(d.page_rank for d in self.pages_data.values())
                self.console.print(f"[blue]üèÜ –°—Ä–µ–¥–Ω–∏–π PageRank: {avg_pagerank:.4f}, –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π: {max_pagerank:.4f}[/blue]")
            
            self.console.print("\n[blue]üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã:[/blue]")
            reports = [
                "seo_–æ—Ç—á–µ—Ç_–æ—Å–Ω–æ–≤–Ω–æ–π.xlsx",
                "seo_–æ—Ç—á–µ—Ç_pagerank.xlsx", 
                "seo_–æ—Ç—á–µ—Ç_—Å—Ç—Ä—É–∫—Ç—É—Ä–∞_—Å–∞–π—Ç–∞.xlsx",
                "seo_–æ—Ç—á–µ—Ç_–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ_—Å—Å—ã–ª–∫–∏.xlsx",
                "sitemap.xml"
            ]
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è—Ö
            if self.last_save_count > 0:
                self.console.print(f"[yellow]üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {self.last_save_count // self.save_interval} —Ä–∞–∑ (–∫–∞–∂–¥—ã–µ {self.save_interval} —Å—Ç—Ä–∞–Ω–∏—Ü)[/yellow]")
            
            # –£—Å–ª–æ–≤–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
            if any(len(data.images) > 0 for data in self.pages_data.values()):
                reports.append("seo_–æ—Ç—á–µ—Ç_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.xlsx")
            
            if any(data.duplicate_content for data in self.pages_data.values()):
                reports.append("seo_–æ—Ç—á–µ—Ç_–¥—É–±–ª–∏–∫–∞—Ç—ã.xlsx")
            
            if self.redirects:
                reports.append("seo_–æ—Ç—á–µ—Ç_—Ä–µ–¥–∏—Ä–µ–∫—Ç—ã.xlsx")
            
            if self.error_urls or self.not_found_urls:
                reports.append("seo_–æ—Ç—á–µ—Ç_–æ—à–∏–±–∫–∏.xlsx")
            
            reports.append(self.error_log_file)
            
            for report in reports:
                self.console.print(f"  ‚Ä¢ {report}")
        
        except KeyboardInterrupt:
            self.console.print("\n[yellow]‚ö†Ô∏è –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º[/yellow]")
            if self.pages_data:
                self.console.print("[blue]üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...[/blue]")
                await self.export_results()
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
            self.log_error(error_msg)
            self.console.print(f"[red]‚ùå {error_msg}[/red]")
        finally:
            await connector.close()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    website_url = input("–í–≤–µ–¥–∏—Ç–µ URL —Å–∞–π—Ç–∞ –¥–ª—è SEO –∞–Ω–∞–ª–∏–∑–∞: ")
    scanner = SEOFrogScanner(website_url)
    
    # –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    print(f"\nüîß –¢–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
    print(f"  ‚Ä¢ –¢–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω: {scanner.config['main_domain_only']}")
    print(f"  ‚Ä¢ –†–∞—Å—á–µ—Ç PageRank: {scanner.config['calculate_pagerank']}")
    print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞: {scanner.config['max_depth']}")
    print(f"  ‚Ä¢ –ú–∏–Ω–∏–º—É–º —Å–ª–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {scanner.config['min_word_count']}")
    
    # –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    asyncio.run(scanner.run())